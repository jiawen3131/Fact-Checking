{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe63081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "subscription_key =  #input subscription key\n",
    "assert subscription_key\n",
    "cx = #input cx\n",
    "assert cx\n",
    "\n",
    "search_url = \"https://www.googleapis.com/customsearch/v1?\"\n",
    "import requests\n",
    "\n",
    "headers={\"key\": subscription_key, \"cx\": cx}\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def grab_web_text(search_query, max_sentences):\n",
    "    params = {\"key\": subscription_key, \"cx\": cx, \"q\": search_query.lower(), \"num\": max_sentences}\n",
    "    response = requests.get(search_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    urls = []\n",
    "    snippets = []\n",
    "    titles = []\n",
    "    if \"items\" not in search_results:\n",
    "        snippets = [search_query] * max_sentences\n",
    "        return urls, snippets\n",
    "    for v in search_results[\"items\"]:\n",
    "        if not isinstance(v['link'], str):\n",
    "            v['link'] = v['link'].decode('utf-8')\n",
    "        if not isinstance(v['title'], str):\n",
    "            v['title'] = v['title'].decode('utf-8')\n",
    "\n",
    "        urls.append(v['link'])\n",
    "        titles.append(v['title'])\n",
    "\n",
    "        if not isinstance(v[\"snippet\"],str):\n",
    "            v[\"snippet\"] = v[\"snippet\"].encode(\"utf-8\")\n",
    "        #if \"datePublished\" in v:\n",
    "        #    print (v[\"datePublished\"])\n",
    "\n",
    "        snippets.append(v[\"snippet\"].replace(\"<b>\",\"\").replace(\"</b>\",\"\").replace(\"\\n\",\"\"))\n",
    "        \n",
    "    \n",
    "    return urls, snippets, titles\n",
    "\n",
    "def live_crawl(id, query, limit):\n",
    "    print ('Querying: {} ...'.format(query))\n",
    "\n",
    "    js = {}\n",
    "    urls, snippets, titles = grab_web_text(query, limit)\n",
    "\n",
    "    js['id'] = id\n",
    "    js['claim'] = query.lower()\n",
    "    js['snippets'] = snippets\n",
    "    js['titles'] = titles\n",
    "    js['urls'] = urls\n",
    "\n",
    "    return js\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--query_path', type=str, default='input.txt')\n",
    "    parser.add_argument('--output_path', type=str, default='output.json')\n",
    "    parser.add_argument('--limit', type=int, default=10)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    with open(args.query_path, 'r') as f:\n",
    "        query = f.read().splitlines()\n",
    "    \n",
    "    count=0\n",
    "    all_records = []\n",
    "    for q in query:\n",
    "        js = live_crawl(count, q, args.limit)\n",
    "        all_records.append(js)\n",
    "        count += 1\n",
    "\n",
    "    records = {'records': all_records}\n",
    "    with open(args.output_path, 'w+') as g:\n",
    "        g.write(json.dumps(records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
