{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e31d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Your storage is almost full (90%). … Soon you won't be able to upload new files to Drive and send or receive emails in Gmail.Learn more\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from csv import reader\n",
    "import timeout_decorator\n",
    "import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "import nltk\n",
    "import re\n",
    "import datefinder\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "\n",
    "subscription_key = #input subscription key\n",
    "assert subscription_key\n",
    "cx = #input cx\n",
    "assert cx\n",
    "\n",
    "search_url = \"https://www.googleapis.com/customsearch/v1?\"\n",
    "import requests\n",
    "\n",
    "headers={\"key\": subscription_key, \"cx\": cx}\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def transform(url):\n",
    "    if url.split('.')[-1]=='pdf':\n",
    "        full_text.append(' ')\n",
    "        return \n",
    "    else:\n",
    "        try:\n",
    "            r = requests.get(str(url),timeout=2)\n",
    "            full_text.append(extract_content(r))\n",
    "            return\n",
    "        except:\n",
    "            full_text.append(' ')\n",
    "            return\n",
    "\n",
    "def extract_content(req):\n",
    "    snippet = []\n",
    "    for string in BeautifulSoup(req.text, \"html.parser\").stripped_strings:\n",
    "        snippet.append(string)\n",
    "    \n",
    "    if len(snippet)==0:\n",
    "        return ' '\n",
    "    else:\n",
    "        return ' '.join(snippet)\n",
    "\n",
    "def date_snippet(snippet):\n",
    "    #dictionary of months\n",
    "    num_months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    alp_months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    month_dict = dict(zip(num_months,alp_months))\n",
    "    #print(month_dict)\n",
    "\n",
    "    # a generator will be returned by the datefinder module. I'm typecasting it to a list. Please read the note of caution provided at the bottom.\n",
    "    matches = list(datefinder.find_dates(snippet))\n",
    "\n",
    "    if len(matches) > 0:\n",
    "        # date returned will be a datetime.datetime object. here we are only using the first match.\n",
    "        date = matches[0]\n",
    "        year = date.strftime(\"%Y\")\n",
    "        month = date.strftime(\"%m\")\n",
    "        day = date.strftime(\"%d\")\n",
    "        if day[0] == '0':\n",
    "            day = day.replace(\"0\",\"\",1)\n",
    "        remove_date = str(month_dict[str(month)] + ' ' + day + ',' + ' ' + year)\n",
    "        #print(remove_date in test[2])\n",
    "        return remove_date\n",
    "\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def clean_data(snippet):\n",
    "    exception = \". . .:(),'\" #do not remove ...\n",
    "    cleaned_test = re.sub(r'[^\\w' + exception + ']', ' ', snippet)\n",
    "    #print(cleaned_test)\n",
    "    date_to_remove = date_snippet(snippet)\n",
    "    if date_to_remove in cleaned_test:\n",
    "        cleaned_test = cleaned_test.replace(date_to_remove, \"\")\n",
    "        #print(\"pass\")\n",
    "        \n",
    "    cleaned_test = cleaned_test.replace(\"...\",\"\",1)\n",
    "    cleaned_test = cleaned_test.lstrip()\n",
    "    if cleaned_test[0] == \".\":\n",
    "        cleaned_test = cleaned_test.replace(\"...\",\"\",1)\n",
    "    cleaned_test = cleaned_test.strip()\n",
    "    cleaned_test = \" \".join(re.split(\"\\s+\", cleaned_test, flags=re.UNICODE))\n",
    "    \" \".join(cleaned_test.split())\n",
    "    return cleaned_test\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def grab_web_text(search_query, max_sentences):\n",
    "    params = {\"key\": subscription_key, \"cx\": cx, \"q\": search_query.lower(), \"num\": max_sentences}\n",
    "    response = requests.get(search_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    urls = []\n",
    "    snippets = []\n",
    "    titles = []\n",
    "    if \"items\" not in search_results:\n",
    "        snippets = [search_query] * max_sentences\n",
    "        return urls, snippets\n",
    "    for v in search_results[\"items\"]:\n",
    "        if not isinstance(v['link'], str):\n",
    "            v['link'] = v['link'].decode('utf-8')\n",
    "        if not isinstance(v['title'], str):\n",
    "            v['title'] = v['title'].decode('utf-8')\n",
    "\n",
    "        urls.append(v['link'])\n",
    "        titles.append(v['title'])\n",
    "\n",
    "        if not isinstance(v[\"snippet\"],str):\n",
    "            v[\"snippet\"] = v[\"snippet\"].encode(\"utf-8\")\n",
    "        #if \"datePublished\" in v:\n",
    "        #    print (v[\"datePublished\"])\n",
    "\n",
    "        snippets.append(v[\"snippet\"].replace(\"<b>\",\"\").replace(\"</b>\",\"\").replace(\"\\n\",\"\"))\n",
    "        \n",
    "    \n",
    "    return urls, snippets, titles\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def live_crawl(id, query, limit):\n",
    "    print ('Querying: {} ...'.format(query))\n",
    "\n",
    "    js = {}\n",
    "    urls, snippets, titles = grab_web_text(query, limit)\n",
    "\n",
    "    js['id'] = id\n",
    "    js['claim'] = query.lower()\n",
    "    js['snippets'] = snippets\n",
    "    js['titles'] = titles\n",
    "    js['urls'] = urls\n",
    "\n",
    "    return js\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--query_path', type=str, default='input.txt')\n",
    "    parser.add_argument('--output_path', type=str, default='output.json')\n",
    "    parser.add_argument('--limit', type=int, default=10)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    with open(args.query_path, 'r') as f:\n",
    "        query = f.read().splitlines()\n",
    "    \n",
    "    count_query=0\n",
    "    all_records = []\n",
    "    for q in query:\n",
    "        print ('Querying: {} ...'.format(q))\n",
    "        urls, snippets, titles = grab_web_text(q, args.limit)\n",
    "        \n",
    "        complete_snippets = []\n",
    "        full_text = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "          executor.map(transform, urls)\n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        while count < len(urls):\n",
    "            extracted_snippet = []\n",
    "            url = urls[count]\n",
    "            incomp_snippet = snippets[count]\n",
    "            full_content = full_text[count]\n",
    "\n",
    "            cleaned_snippet = clean_data(incomp_snippet).lower().split('...')\n",
    "            cleaned_snippet = list(filter(lambda string : string.strip(), cleaned_snippet))\n",
    "            sentences_to_complete = len(cleaned_snippet)\n",
    "            \n",
    "            clean_text_sentences = split_into_sentences(full_content)\n",
    "            \n",
    "            sentences_completed = 0\n",
    "\n",
    "            for sent in clean_text_sentences:\n",
    "\n",
    "                if sentences_completed == sentences_to_complete:\n",
    "                    break\n",
    "\n",
    "                if len(sent) < len(cleaned_snippet[sentences_completed]):\n",
    "                    continue\n",
    "\n",
    "                actual_sentence = sent\n",
    "                test_sentence = ''.join(filter(str.isalnum, sent.lower()))\n",
    "                cleaned_snippet_sent = ''.join(filter(str.isalnum, cleaned_snippet[sentences_completed]))\n",
    "\n",
    "                if cleaned_snippet_sent in test_sentence:\n",
    "                    extracted_snippet.append(actual_sentence)\n",
    "                    sentences_completed += 1\n",
    "                    \n",
    "            if len(extracted_snippet)==0:\n",
    "                complete_snippets.append(' '.join(cleaned_snippet))\n",
    "            \n",
    "            else:\n",
    "                print(\"#####Successfully retrieved!###########\")\n",
    "                completed_snippet = ' '.join(extracted_snippet)\n",
    "                print(\"Complete snippet: \", completed_snippet)\n",
    "                print()\n",
    "                print('-'*100)\n",
    "                complete_snippets.append(completed_snippet)\n",
    "            \n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "              \n",
    "        js = {}\n",
    "        js['id'] = count\n",
    "        js['claim'] = q.lower()\n",
    "        js['snippets'] = complete_snippets\n",
    "        js['titles'] = titles\n",
    "        js['urls'] = urls\n",
    "        all_records.append(js)\n",
    "          \n",
    "        count_query += 1\n",
    "\n",
    "    records = {'records': all_records}\n",
    "    with open(args.output_path, 'w+') as g:\n",
    "        g.write(json.dumps(records))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Total time taken: \", end-start)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
